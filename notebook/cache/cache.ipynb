{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env VECLIB_MAXIMUM_THREADS=1\n",
    "%env MKL_NUM_THREADS=1\n",
    "%env NUMEXPR_NUM_THREADS=1\n",
    "%env OMP_NUM_THREADS=1\n",
    "\n",
    "!make clean\n",
    "\n",
    "import numpy as np\n",
    "#np.show_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cache optimization\n",
    "\n",
    "1. How cache works and its importance to performance\n",
    "2. Stride analysis\n",
    "3. Tiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory hierarchy\n",
    "\n",
    "To show how cache works and its importance to performance, see table below:\n",
    "\n",
    "| Type | Latency (cycles) |\n",
    "| -- | -- |\n",
    "| CPU registers | 0 |\n",
    "| L1 cache | 4 |\n",
    "| L2 cache | 10 |\n",
    "| L3 cache | 50 |\n",
    "| Main memory | 200 |\n",
    "| Disk / drive | 100,000 |\n",
    "\n",
    "(Excerpt from Figure 6.23 in CS:APP (3/e).)\n",
    "\n",
    "Remarks:\n",
    "* Disk (or SSD) is slow.  Don't do I/O unless absolutely necessary.\n",
    "* CPU instruction is 100x faster than memory access.\n",
    "* Cache hides the memory latency.\n",
    "\n",
    "Nothing is fast without cache."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How cache works\n",
    "\n",
    "Assume we have a main memory of 64 bytes, and a cache of 8 bytes.\n",
    "\n",
    "| Access # | Decimal addr | Binary addr | Hit or miss | Cache block (binary) |\n",
    "| -- | -- | -- | -- | -- |\n",
    "| 1 | 22 | 10110 | miss (cold) | 110 |\n",
    "| 2 | 26 | 11010 | miss (cold) | 010 |\n",
    "| 3 | 22 | 10110 | hit | 110 |\n",
    "| 4 | 26 | 11010 | hit | 010 |\n",
    "| 5 | 16 | 10000 | miss (cold) | 000 |\n",
    "| 6 | 18 | 10010 | miss (conflict) | 010 |\n",
    "| 7 | 26 | 11010 | miss (conflict) | 010 |\n",
    "\n",
    "This is a direct-map cache.  To reduce conflict misses, we may use multi-way set associativity.  2-, 4-, or 8-way set associative cache is commonly used.  Full associativity is too expensive in circuit implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cache block (line)\n",
    "\n",
    "A cache block usually contains more than one byte or word.  In x86, the block (line) size is 64 bytes.  That is, when loading data from main memory to cache, it's done once a block, rather than byte by byte.\n",
    "\n",
    "```cpp\n",
    "// Sequential.\n",
    "for (int i=0; i<nelem; ++i) { arr[i] = i; }\n",
    "sw.lap();\n",
    "for (int i=0; i<nelem; ++i) { arr[i] *= 3; }\n",
    "elapsed = sw.lap();\n",
    "\n",
    "// Skipping 2.\n",
    "for (int i=0; i<nelem; ++i) { arr[i] = i; }\n",
    "sw.lap();\n",
    "for (int i=0; i<nelem; i+=2) { arr[i] *= 3; }\n",
    "elapsed = sw.lap();\n",
    "\n",
    "// ... 4, 8, 16, ... 1024.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how main memory (dram) access dominates runtime.\n",
    "!make 01_skip_access ; ./01_skip_access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locality\n",
    "\n",
    "While coding we usually don't have a lot of time to do detailed cache analysis.  Instead, we keep in mind that the code runs faster when it's more compact.  This is the concept of locality.\n",
    "\n",
    "There are two kinds of locality: temporal and spatial.  Temporal locality means that a fixed address is reused in the near future.  Spatial locality means that the addresses close to the current address is reused in the near future.  The better locality, of either kind, improves the performance.  And the cache hierarchy is why locality works.\n",
    "\n",
    "To take advantage of locality, programmers analyze by using \"strides\".  A stride is the number of indexes to elements to slide when accessing the data in arrays.  The most basic striding is sequential access, or the 1-stride.  Similarly, we may have n-strides.  The larger the stride is, the worse the locality.\n",
    "\n",
    "Recall that x86 uses 64-byte cache blocks, and a double-precision floating point takes 8 bytes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrate by matrix-vector multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how striding affects runtime.\n",
    "!make 02_locality ; ./02_locality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While writing programs, it's much easier to know the stride than analyzing the cache behavior.  The latter, in many scenarios, is prohibitively difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Array majoring\n",
    "\n",
    "Array majoring is directly related to locality.  The difference in the performance of matrix-vector multiplication is show for row- and column-majoring arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dim = 10000\n",
    "float_rmajor = np.arange(dim*dim, dtype='float64').reshape((dim,dim))\n",
    "float_cmajor = float_rmajor.T.copy().T\n",
    "vec = np.arange(dim, dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "res_float_rmajor = np.dot(float_rmajor, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "res_float_cmajor = np.dot(float_cmajor, vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integer matrix-vector multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dim = 10000\n",
    "int_rmajor = np.arange(dim*dim, dtype='int64').reshape((dim,dim))\n",
    "int_cmajor = int_rmajor.T.copy().T\n",
    "vec = np.arange(dim, dtype='int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "res_int_rmajor = np.dot(int_rmajor, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "res_int_cmajor = np.dot(int_cmajor, vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance difference of integer arrays is much larger than floating-point arrays.  Note that `double` and `int64` both take 8 bytes.  Reason: LAPACK / MKL / vecLib.\n",
    "\n",
    "For the same reason, the floating-point multiplication is slightly faster than the integer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiling\n",
    "\n",
    "This is a naive implementation of matrix-matrix multiplication:\n",
    "\n",
    "```cpp\n",
    "for (size_t i=0; i<mat1.nrow(); ++i)\n",
    "{\n",
    "    for (size_t k=0; k<mat2.ncol(); ++k)\n",
    "    {\n",
    "        double v = 0;\n",
    "        for (size_t j=0; j<mat1.ncol(); ++j)\n",
    "        {\n",
    "            v += mat1(i,j) * mat2(j,k);\n",
    "        }\n",
    "        ret(i,k) = v;\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "The matrices are row-major.  The stride for the second matrix is `ncol2`, so it doesn't have good locality.  This naive implementation is clear, but the performance is bad.\n",
    "\n",
    "Matrix-matrix multiplication is one of the most important problems for numerical calculation, and there are many techniques available for making it fast.  Most if not all of them are about hiding the memory access latency.  Tiling is a basic technique that delivers impressive speed-up by reordering the calculation and making it cache-friendly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how tiling improves runtime performance.\n",
    "!make 03_matrix_matrix ; ./03_matrix_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. Consult the data sheet of one x86 CPU and one Arm CPU.  Make a table for the line size of each of the cache levels, and compare the difference between the two CPUs.\n",
    "2. Write a program that uses tiling to speed up matrix-matrix multiplication, and do not require the matrix dimension to be multiples of the tile size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "* Gallery of Processor Cache Effects: http://igoro.com/archive/gallery-of-processor-cache-effects/\n",
    "* Lecture Notes of Applications of Parallel Computers by David Bindel: https://www.cs.cornell.edu/~bindel/class/cs5220-s10/slides/lec03.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
