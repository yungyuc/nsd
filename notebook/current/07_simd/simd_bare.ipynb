{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!make -C code clean\n",
    "\n",
    "import os, platform\n",
    "import numpy as np\n",
    "#np.show_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIMD (vector processing)\n",
    "\n",
    "1. Types of parallelism\n",
    "   1. Shared-memory parallelism\n",
    "   2. Distributed-memory parallelism\n",
    "   3. Vector processing\n",
    "2. SIMD instructions\n",
    "   1. CPU capabilities\n",
    "   2. x86 intrinsic functions\n",
    "   3. Symbol table\n",
    "   4. Inspect assembly: 1, 3, 5 multiplications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of parallelism\n",
    "\n",
    "The popular computer architecture is based on sequential processing.  The most fundamental processing unit executes instructions one by one.\n",
    "\n",
    "<center><img src=\"image/architecture.png\" alt=\"Common computer architecture\" /></center>\n",
    "\n",
    "If we assume the processor can only perform sequential processing, we need to use multiple processors to perform parallel processing.  Differentiated by the memory access, the parallelism can be broadly set to two categories:\n",
    "\n",
    "* Shared-memory parallel processing\n",
    "* Distributed-memory parallel processing\n",
    "\n",
    "## Shared-memory parallel processing\n",
    "\n",
    "<br />\n",
    "<center><img src=\"image/shared.png\" alt=\"Shared-memory parallelism\" /></center>\n",
    "\n",
    "## Distributed-memory parallel processing\n",
    "\n",
    "<br />\n",
    "<center><img src=\"image/distributed.png\" alt=\"Distributed-memory parallelism\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector processing\n",
    "\n",
    "When the parallelism happens in the processor (one processing unit or core), it is usually done once for a single instruction with multiple data (SIMD).  It has also been called vector processing (it's an illustrative name).\n",
    "\n",
    "Before showing what is vector processing, let us see the ordinary scalar execution:\n",
    "\n",
    "<center><img src=\"image/scalar.png\" width=\"40%\" alt=\"Scalar execution\" /></center>\n",
    "\n",
    "The vector execution uses a wider register so that it can perform an operation for multiple data at once:\n",
    "\n",
    "<center><img src=\"image/vector.png\" width=\"80%\" alt=\"Vector execution\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check CPU capabilities\n",
    "\n",
    "To take advantage of SIMD, we will need to inspect the CPU instructions, or the assembly.  But most of the time we stay in high-level languages.  The way we deal with the assembly is to get familiar with the instructions, e.g., using [x86 and amd64 instruction reference](https://www.felixcloutier.com/x86/).\n",
    "\n",
    "x86 provides a series of SIMD instructions, including\n",
    "\n",
    "* 64-bit: MMX\n",
    "* 128-bit: SSE, SSE2, SSE3, SSE4, SSE4.1, SSE4.2 (streaming simd extension)\n",
    "* 256-bit: AVX, AVX2 (advanced vector extension)\n",
    "* 512-bit: AVX-512\n",
    "\n",
    "Recent processors usually are equipped with AVX2, which was released with Haswell in 2013.  Before asking the compiler to use the specific instruction set, query the operating system for the cpu capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Check on\", platform.system())\n",
    "if 'Linux' == platform.system():\n",
    "    # check whether your cpu supports avx2 on linux\n",
    "    !grep flags /proc/cpuinfo\n",
    "elif 'Darwin' == platform.system():\n",
    "    # check whether your cpu supports avx2 on mac\n",
    "    !sysctl -a | grep machdep.cpu.*features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# x86 intrinsic functions\n",
    "\n",
    "Major compilers provide header files for using the intrinsic functions that can be directly translated into the SIMD instructions:\n",
    "\n",
    "* `<mmintrin.h>`: MMX\n",
    "* `<xmmintrin.h>`: SSE\n",
    "* `<emmintrin.h>`: SSE2\n",
    "* `<pmmintrin.h>`: SSE3\n",
    "* `<tmmintrin.h>`: SSSE3\n",
    "* `<smmintrin.h>`: SSE4.1\n",
    "* `<nmmintrin.h>`: SSE4.2\n",
    "* `<ammintrin.h>`: SSE4A\n",
    "* `<immintrin.h>`: AVX\n",
    "* `<zmmintrin.h>`: AVX512\n",
    "\n",
    "You may also use `<x86intrin.h>` which includes everything.\n",
    "\n",
    "With the intrinsic functions, programmers don't need to really write assembly, and can stay in the high-level languages most of the time.  The intrinsic functions correspond to x86 instructions.  An example of using it:\n",
    "\n",
    "```cpp\n",
    "__m256 * ma = (__m256 *) (&a[i*width]);\n",
    "__m256 * mb = (__m256 *) (&b[i*width]);\n",
    "__m256 * mr = (__m256 *) (&r[i*width]);\n",
    "*mr = _mm256_mul_ps(*ma, *mb);\n",
    "```\n",
    "\n",
    "**Intel intrinsic guide**: Intel maintains a website to show the available intrinsics: https://software.intel.com/sites/landingpage/IntrinsicsGuide/ .  Consult and remember it when needed.\n",
    "\n",
    "Using intrinsics and SIMD for optimization is a tedious process.  The materials presented here are not a complete guide to you, but show you one way to study and measure the benefits.  The measurement is important to assess whether or not you need the optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the example, `01_mul/mul.cpp`, to show how to use the 256-bit-wide AVX to perform vector multiplication for 8 single-precision floating-point values.\n",
    "\n",
    "```cpp\n",
    "constexpr const size_t width = 8;\n",
    "constexpr const size_t repeat = 1024 * 1024;\n",
    "constexpr const size_t nelem = width * repeat;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time the difference between the loop and the simd/avx versions\n",
    "!make -C code/01_mul run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symbol table\n",
    "\n",
    "I use [radare2](https://rada.re/n/) to inspect the assembly of the generated image.  Before really checking the assembly, we need to identify what functions to be inspected from the symbol table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the symbol table\n",
    "!make -C code/01_mul r2sym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate the effect of different ratio of calculations to memory access, I use 3 sets of multiplication.  The first set uses 1 multiplication:\n",
    "\n",
    "```cpp\n",
    "void multiply1_loop(float* a, float* b, float* r)\n",
    "{\n",
    "    for (size_t i=0; i<repeat*width; i+=width)\n",
    "    {\n",
    "        for (size_t j=i; j<i+width; ++j)\n",
    "        {\n",
    "            r[j] = a[j] * b[j];\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "void multiply1_simd(float* a, float* b, float* r)\n",
    "{\n",
    "    for (size_t i=0; i<repeat; ++i)\n",
    "    {\n",
    "        __m256 * ma = (__m256 *) (&a[i*width]);\n",
    "        __m256 * mb = (__m256 *) (&b[i*width]);\n",
    "        __m256 * mr = (__m256 *) (&r[i*width]);\n",
    "        *mr = _mm256_mul_ps(*ma, *mb);\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 multiplication with loop\n",
    "!make -C code/01_mul r2 NAME=multiply1_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 multiplication with simd/avx\n",
    "!make -C code/01_mul r2 NAME=multiply1_simd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second set uses 3 multiplications:\n",
    "\n",
    "```cpp\n",
    "void multiply3_loop(float* a, float* b, float* r)\n",
    "{\n",
    "    for (size_t i=0; i<repeat*width; i+=width)\n",
    "    {\n",
    "        for (size_t j=i; j<i+width; ++j)\n",
    "        {\n",
    "            r[j] = a[j] * a[j];\n",
    "            r[j] *= b[j];\n",
    "            r[j] *= b[j];\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "void multiply3_simd(float* a, float* b, float* r)\n",
    "{\n",
    "    for (size_t i=0; i<repeat; ++i)\n",
    "    {\n",
    "        __m256 * ma = (__m256 *) (&a[i*width]);\n",
    "        __m256 * mb = (__m256 *) (&b[i*width]);\n",
    "        __m256 * mr = (__m256 *) (&r[i*width]);\n",
    "        *mr = _mm256_mul_ps(*ma, *ma);\n",
    "        *mr = _mm256_mul_ps(*mr, *mb);\n",
    "        *mr = _mm256_mul_ps(*mr, *mb);\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 multiplication with loop\n",
    "!make -C code/01_mul r2 NAME=multiply3_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 multiplication with simd/avx\n",
    "!make -C code/01_mul r2 NAME=multiply3_simd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third (last) set uses 5 multiplications:\n",
    "\n",
    "```cpp\n",
    "void multiply5_loop(float* a, float* b, float* r)\n",
    "{\n",
    "    for (size_t i=0; i<repeat*width; i+=width)\n",
    "    {\n",
    "        for (size_t j=i; j<i+width; ++j)\n",
    "        {\n",
    "            r[j] = a[j] * a[j];\n",
    "            r[j] *= a[j];\n",
    "            r[j] *= b[j];\n",
    "            r[j] *= b[j];\n",
    "            r[j] *= b[j];\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "void multiply5_simd(float* a, float* b, float* r)\n",
    "{\n",
    "    for (size_t i=0; i<repeat; ++i)\n",
    "    {\n",
    "        __m256 * ma = (__m256 *) (&a[i*width]);\n",
    "        __m256 * mb = (__m256 *) (&b[i*width]);\n",
    "        __m256 * mr = (__m256 *) (&r[i*width]);\n",
    "        *mr = _mm256_mul_ps(*ma, *ma);\n",
    "        *mr = _mm256_mul_ps(*mr, *ma);\n",
    "        *mr = _mm256_mul_ps(*mr, *mb);\n",
    "        *mr = _mm256_mul_ps(*mr, *mb);\n",
    "        *mr = _mm256_mul_ps(*mr, *mb);\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 multiplication with loop\n",
    "!make -C code/01_mul r2 NAME=multiply5_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 multiplication with simd/avx\n",
    "!make -C code/01_mul r2 NAME=multiply5_simd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!make -C code/03_omp run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!env OMP_NUM_THREADS=1 make -C code/03_omp run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!env OMP_NUM_THREADS=5 make -C code/03_omp run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. Replace the single-precision floating-point vector type `__m256` with the double-precision floating-point vector type `__m256d` in the example, and compare the performance with the sinple-precision version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. Crunching Numbers with AVX and AVX2 (AVX tutorials): https://www.codeproject.com/Articles/874396/Crunching-Numbers-with-AVX-and-AVX\n",
    "2. Agner Fog (Agner's website): https://www.agner.org\n",
    "\n",
    "   * Instruction table (latency information): https://www.agner.org/optimize/instruction_tables.pdf\n",
    "   * Software optimization resources: https://www.agner.org/optimize/\n",
    "3. x86 and amd64 instruction reference (unofficial) by FÃ©lix Cloutier: https://www.felixcloutier.com/x86/\n",
    "4. Intel Intrinsics Guide: https://software.intel.com/sites/landingpage/IntrinsicsGuide/\n",
    "5. Computer Organization and Assembly Languages by Yung-Yu Chuang, NTU: https://www.csie.ntu.edu.tw/~cyy/courses/assembly/12fall/news/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
