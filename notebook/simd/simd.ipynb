{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env VECLIB_MAXIMUM_THREADS=1\n",
    "%env MKL_NUM_THREADS=1\n",
    "%env NUMEXPR_NUM_THREADS=1\n",
    "%env OMP_NUM_THREADS=1\n",
    "\n",
    "!make clean\n",
    "\n",
    "import os, platform\n",
    "import numpy as np\n",
    "#np.show_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIMD: vector processing\n",
    "\n",
    "1. Types of parallelism.\n",
    "2. x86 intrinsic funcions.\n",
    "3. Inspect assembly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of parallelism\n",
    "\n",
    "The popular computer architecture is based on sequential processing.  The most fundamental processing unit executes instructions one by one.\n",
    "\n",
    "If we assume the processor can only perform sequantial processing, we need to use multiple processors to perform parallel processing.  Differentiated by the memory access, the parallelism can be broadly set to two categories:\n",
    "\n",
    "* Shared-memory parallel processing\n",
    "* Distributed-memory parallel processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector processing\n",
    "\n",
    "When the parallelism happens in the processor (one processing unit or core), it is usually done once for a single instruction with multiple data (SIMD).  It has also been called vector processing.  Vector processing is an illustrative name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check CPU capabilities\n",
    "\n",
    "x86 provides a series of SIMD instructions, including\n",
    "\n",
    "* 64-bit: MMX\n",
    "* 128-bit: SSE, SSE2, SSE3, SSE4, SSE4.1, SSE4.2 (streaming simd extension)\n",
    "* 256-bit: AVX, AVX2 (advanced vector extension)\n",
    "* 512-bit: AVX-512\n",
    "\n",
    "Recent processors usually are equipped with AVX2, which was released with Haswell in 2013.  Before asking the compiler to use the specific instruction set, query the operating system for the cpu capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Check on\", platform.system())\n",
    "if 'Linux' == platform.system():\n",
    "    # check whether your cpu supports avx2 on linux\n",
    "    !grep flags /proc/cpuinfo\n",
    "elif 'Darwin' == platform.system():\n",
    "    # check whether your cpu supports avx2 on mac\n",
    "    !sysctl -a | grep machdep.cpu.*features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# x86 intrinsic functions\n",
    "\n",
    "Major compilers provide header files for using the intrinsic functions that can be directly translated into the SIMD instructions:\n",
    "\n",
    "* `<mmintrin.h>`: MMX\n",
    "* `<xmmintrin.h>`: SSE\n",
    "* `<emmintrin.h>`: SSE2\n",
    "* `<pmmintrin.h>`: SSE3\n",
    "* `<tmmintrin.h>`: SSSE3\n",
    "* `<smmintrin.h>`: SSE4.1\n",
    "* `<nmmintrin.h>`: SSE4.2\n",
    "* `<ammintrin.h>`: SSE4A\n",
    "* `<immintrin.h>`: AVX\n",
    "* `<zmmintrin.h>`: AVX512\n",
    "\n",
    "You may also use `<x86intrin.h>` which includes everything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first example, `01_mul/mul.cpp`, shows how to use the 256-bit-wide AVX to perform vector multiplication for 8 single-precision floating-point values.\n",
    "\n",
    "```cpp\n",
    "constexpr const size_t width = 8;\n",
    "constexpr const size_t repeat = 1024 * 1024;\n",
    "constexpr const size_t nelem = width * repeat;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time the difference between the loop and the simd/avx versions\n",
    "!g++ -std=c++17 -g -O3 -m64 -mavx 01_mul/mul.cpp -o 01_mul/mul ; 01_mul/mul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the assembly\n",
    "\n",
    "I use radare2 to inspect the assembly of the generated image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the symbol table\n",
    "!r2 -Aqc \"e scr.color=0 ; afl\" 01_mul/mul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate the effect of different ratio of calculations to memory access, I use 3 sets of multiplication.  The first set uses 1 multiplication:\n",
    "\n",
    "```cpp\n",
    "void multiply1_loop(float* a, float* b, float* r)\n",
    "{\n",
    "    for (size_t i=0; i<repeat*width; i+=width)\n",
    "    {\n",
    "        for (size_t j=i; j<i+width; ++j)\n",
    "        {\n",
    "            r[j] = a[j] * b[j];\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "void multiply1_simd(float* a, float* b, float* r)\n",
    "{\n",
    "    for (size_t i=0; i<repeat; ++i)\n",
    "    {\n",
    "        __m256 * ma = (__m256 *) (&a[i*width]);\n",
    "        __m256 * mb = (__m256 *) (&b[i*width]);\n",
    "        __m256 * mr = (__m256 *) (&r[i*width]);\n",
    "        *mr = _mm256_mul_ps(*ma, *mb);\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 multiplication with loop\n",
    "!r2 -Aqc \"e scr.color=0 ; s sym.multiply1_loop_float__float__float ; pdf\" 01_mul/mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 multiplication with simd/avx\n",
    "!r2 -Aqc \"e scr.color=0 ; s sym.multiply1_simd_float__float__float ; pdf\" 01_mul/mul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second set uses 3 multiplications:\n",
    "\n",
    "```cpp\n",
    "void multiply3_loop(float* a, float* b, float* r)\n",
    "{\n",
    "    for (size_t i=0; i<repeat*width; i+=width)\n",
    "    {\n",
    "        for (size_t j=i; j<i+width; ++j)\n",
    "        {\n",
    "            r[j] = a[j] * a[j];\n",
    "            r[j] *= b[j];\n",
    "            r[j] *= b[j];\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "void multiply3_simd(float* a, float* b, float* r)\n",
    "{\n",
    "    for (size_t i=0; i<repeat; ++i)\n",
    "    {\n",
    "        __m256 * ma = (__m256 *) (&a[i*width]);\n",
    "        __m256 * mb = (__m256 *) (&b[i*width]);\n",
    "        __m256 * mr = (__m256 *) (&r[i*width]);\n",
    "        *mr = _mm256_mul_ps(*ma, *ma);\n",
    "        *mr = _mm256_mul_ps(*mr, *mb);\n",
    "        *mr = _mm256_mul_ps(*mr, *mb);\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 multiplication with loop\n",
    "!r2 -Aqc \"e scr.color=0 ; s sym.multiply3_loop_float__float__float ; pdf\" 01_mul/mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 multiplication with simd/avx\n",
    "!r2 -Aqc \"e scr.color=0 ; s sym.multiply3_simd_float__float__float ; pdf\" 01_mul/mul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third (last) set uses 5 multiplications:\n",
    "\n",
    "```cpp\n",
    "void multiply5_loop(float* a, float* b, float* r)\n",
    "{\n",
    "    for (size_t i=0; i<repeat*width; i+=width)\n",
    "    {\n",
    "        for (size_t j=i; j<i+width; ++j)\n",
    "        {\n",
    "            r[j] = a[j] * a[j];\n",
    "            r[j] *= a[j];\n",
    "            r[j] *= b[j];\n",
    "            r[j] *= b[j];\n",
    "            r[j] *= b[j];\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "void multiply5_simd(float* a, float* b, float* r)\n",
    "{\n",
    "    for (size_t i=0; i<repeat; ++i)\n",
    "    {\n",
    "        __m256 * ma = (__m256 *) (&a[i*width]);\n",
    "        __m256 * mb = (__m256 *) (&b[i*width]);\n",
    "        __m256 * mr = (__m256 *) (&r[i*width]);\n",
    "        *mr = _mm256_mul_ps(*ma, *ma);\n",
    "        *mr = _mm256_mul_ps(*mr, *ma);\n",
    "        *mr = _mm256_mul_ps(*mr, *mb);\n",
    "        *mr = _mm256_mul_ps(*mr, *mb);\n",
    "        *mr = _mm256_mul_ps(*mr, *mb);\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 multiplication with loop\n",
    "!r2 -Aqc \"e scr.color=0 ; s sym.multiply5_loop_float__float__float ; pdf\" 01_mul/mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 multiplication with simd/avx\n",
    "!r2 -Aqc \"e scr.color=0 ; s sym.multiply5_simd_float__float__float ; pdf\" 01_mul/mul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intel intrinsics guide\n",
    "\n",
    "Intel maintains a website to show the available intrinsics: https://software.intel.com/sites/landingpage/IntrinsicsGuide/ .  Consult and remember it when needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. Replace the single-precision floating-point vector type `__m256` with the double-precision floating-point vector type `__m256d` in the example, and compare the performance with the sinple-precision version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. Crunching Numbers with AVX and AVX2 (AVX tutorials): https://www.codeproject.com/Articles/874396/Crunching-Numbers-with-AVX-and-AVX\n",
    "2. Agner Fog (Agner's website): https://www.agner.org\n",
    "\n",
    "   * Instruction table (latency information): https://www.agner.org/optimize/instruction_tables.pdf\n",
    "3. x86 and amd64 instruction reference (unofficial) by FÃ©lix Cloutier: https://www.felixcloutier.com/x86/\n",
    "4. Intel Intrinsics Guide: https://software.intel.com/sites/landingpage/IntrinsicsGuide/\n",
    "5. Computer Organization and Assembly Languages by Yung-Yu Chuang, NTU: https://www.csie.ntu.edu.tw/~cyy/courses/assembly/12fall/news/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
